<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>How malicious AI swarms can threaten democracy | Content</title></head><body><nav style=margin-bottom:2em><a href=/>← Home</a></nav><main><article><h1>How malicious AI swarms can threaten democracy</h1><h1 id=how-malicious-ai-swarms-can-threaten-democracy>How malicious AI swarms can threaten democracy</h1><p>The fusion of agentic AI and LLMs marks a new frontier in information warfare.</p><p>Advances in AI offer the prospect of manipulating beliefs and behaviors on a population-wide level [1]. Large language models (LLMs) and autonomous agents [2] now let influence campaigns reach unprecedented scale and precision. Generative tools can expand propaganda output without sacrificing credibility [3] and inexpensively create falsehoods that are rated as more human-like than those written by humans [4, 3]. Techniques meant to refine AI reasoning, such as chain-of-thought prompting, can just as effectively be used to generate more convincing falsehoods. Enabled by these capabilities, a disruptive threat is emerging: swarms of collaborative, malicious AI agents. Fusing LLM reasoning with multi-agent architectures [2], these systems are capable of coordinating autonomously, infiltrating communities, and fabricating consensus efficiently. By adaptively mimicking human social dynamics, they threaten democracy. Because the resulting harms stem from design, commercial incentives, and governance, we prioritize interventions at multiple leverage points, focusing on pragmatic mechanisms over voluntary compliance.</p><p>This risk compounds long-standing vulnerabilities in democratic information ecosystems, already weakened by erosion of rational-critical discourse and a lack of shared reality among citizens. AI swarms are a potent accelerant in this trajectory, though their ultimate impact is not predetermined. Their effects will be shaped by platform design, market incentives, media institutions, and political actors. Here, we distinguish documented trends from projections, indicate where uncertainty remains, and note countervailing dynamics, such as growing public skepticism toward unverified content and a renewed interest in institutional demand for accountable journalism (see supplementary materials).</p><p>AI swarms continue a long history of communication technologies reshaping political power. The advent of the printing press enabled a &ldquo;public sphere&rdquo; and the mass circulation of ideas that challenged state authority. The broadcast era centralized influence in a one-to-many communication model. Here, the public sphere shifted from a site of participation to one of mass media consumption, often exploited by politicians and their parties for national cohesion and mass persuasion. The digital era then fragmented this landscape. By lowering entry barriers, social media platforms enabled many-to-many communication and simultaneously a polarized environment for modern information operations. In this context, online manipulation accelerated, driven increasingly by domestic political elites and parties (now understood to be major drivers of disinformation) alongside foreign state actors. They have targeted events like Brexit and elections in the U.S., Brazil, and the Philippines. Our backdrop is thus not an idealized public sphere, but one strained by decades of technological disruption and democratic backsliding. This has manifested as a sharp decline in public trust in core institutions (including the media, science, and government), thereby corroding the very foundations of evidence-based discourse upon which democratic deliberation depends.</p><p>A prime pre-generative AI example of influence operations is the state-backed, human-driven botnet. During the Russian Internet Research Agency&rsquo;s (IRA) 2016 Twitter operation, only one percent of users saw 70% of its content, with no detectable effects on opinions or turnout [5]. We do not claim that the IRA &ldquo;failed&rdquo; entirely due to technical shortcomings; its objectives also included sowing epistemic uncertainty and distrust. Nevertheless, this example highlights the cost, cadence, and iteration limits inherent to human-operated systems that new developments in AI can help overcome.</p><p>This leap, from human- to AI-driven influence operations, is underway. LLMs generate persuasive, tailored text at scale and have shifted deep-seated beliefs in laboratory settings [6]. Open-source releases further lower access barriers. Consequently, AI-supported election interference is no longer hypothetical. Taiwan&rsquo;s, India&rsquo;s, Indonesia&rsquo;s, and the U.S.&rsquo; 2024 campaigns saw deepfakes, and fabricated news outlets now influence debates. Absent guardrails, LLM-driven swarms can transform sporadic mis- and disinformation into persistent, adaptive manipulation of democratic discourse.</p><h2 id=swarm-capabilities>Swarm Capabilities</h2><p>A malicious AI swarm is a set of AI-controlled agents that (i) maintains persistent identities and memory; (ii) coordinates toward shared objectives while varying tone and content; (iii) adapts in real time to engagement, platform cues, and human responses; (iv) operates with minimal human oversight; and (v) can deploy across platforms. Classic coordinated inauthentic behavior amplifies the spread of information by inflating content frequency and engagement to trigger algorithmic visibility through repetition, manual scheduling, and rigid scripts. Swarms differ by fusing scale, heterogeneity, and real-time adaptation: they can generate organic-looking, context-aware content, sustain coherent narratives across agents, and evolve with feedback. This synthesis, enabled by model-driven generation, memory, and planning, could achieve effects that conventional, human-intensive operations cannot match in speed or cost.</p><p>Recent breakthroughs in multi-agent systems have fused LLM reasoning with agentic memory, planning, and communication [7]. Five advances now matter for influence operations.</p><p>First is the shift from central command to fluid, real-time coordination. A single adversary could operate thousands of AI personas, scheduling content and updating narrative frames across fleets. Local adaptation plus periodic synchronization with a central node blurs the line between command-and-control and emergent &ldquo;hive&rdquo; behavior. If these agent swarms evolve into loosely governed &ldquo;societies,&rdquo; with internal norm formation and division of labor, the challenge shifts from tracing commands to understanding emergent group cognition [8]. These &ldquo;societies&rdquo; may undergo spontaneous or adversarially-induced norm shifts, abandoning engineered constraints for new behavioral patterns via tipping-point effects [8].</p><p>Second, agents can employ systems that map social network structures at scale and infiltrate vulnerable communities with tailored appeals, winning followers [9]. They can identify key communities and beliefs, and track trending topics. This process can be decentralized with global, network-wide efficacy [10]. Equipped with such capabilities, swarms can position for maximum impact and tailor messages to the beliefs and cultural cues of each community, enabling more precise targeting than previous botnets.</p><p>Third, human-level mimicry helps swarms evade detectors that once caught simpler &ldquo;copy-paste&rdquo; bots. Detection of coordinated inauthentic behavior generally relies on activity patterns being suspiciously similar across accounts and, thus, statistically unlikely to be independent [11]. Photorealistic avatars, context-appropriate slang, and heterogeneous posting rhythms can circumvent the synchrony older detectors flag.</p><p>Fourth, swarms may become increasingly self-optimizing, harvesting real-time engagement data, recommender cues, or user feedback in plain language. With sufficient signals, they may run millions of micro-A/B tests, propagate the winning variants at machine speed, and iterate far faster than humans.</p><p>Finally, an around-the-clock presence turns influence into a long-term, low-friction infrastructure. Unlike transient operations, agent swarms can persist, embedding themselves within communities over long timescales and gradually shifting discourse. This persistent influence can drive deeper cultural changes beyond norm shifts, subtly altering a community&rsquo;s language, symbols, and identity [12]. It amplifies other mechanisms described above. In cognitive warfare, AI&rsquo;s relentless operational endurance becomes a weapon against limited human efforts.</p><h2 id=pathways-of-harms-to-democracy>Pathways of Harms to Democracy</h2><p>Emerging capabilities of swarm-driven influence campaigns threaten democracy by shaping public opinion, which leads to cascading harms. These pathways are conditional claims that may materialize, especially where recommenders, ad markets, and moderation practices reward coordinated messaging with weak provenance, and where business models privilege engagement over authenticity (i.e., the currently dominant model of most social media platforms). Emerging counter-trends such as migration to smaller communities and increased reliance on verified outlets may mitigate some harms.</p><p>In today&rsquo;s fragmented information environment, ideological echo chambers offer fertile ground for manipulation. AI swarms are uniquely equipped to exploit this by engineering a synthetic consensus that appears to bridge these divides. They may seed narratives across disparate niches, creating an illusion of majority agreement. They can also boost this illusion by liking posts, making narratives appear widely supported. Citizens then update opinions based on peer norms, more so than evidence. A chorus of seemingly independent voices creates a mirage of bipartisan grassroot consensus with enhanced speed and persuasiveness. The result is deeply embedded manipulation that lets operators nudge public discourse almost invisibly over time.</p><p>This chorus erodes the independence essential to collective intelligence and democracy, already weakened by pervasive social influence operations on contemporary platforms. Beyond social norms, this directly undermines human cognitive information processing. The &ldquo;wisdom of crowds,&rdquo; where aggregated judgments outperform experts, depends critically on independence between judgments. While rudimentary botnets already replicate messages to simulate consensus, swarms of AI agents can do so with far greater sophistication, adaptivity, and contextual awareness. Citizens may then overestimate the informational value of this artificial consensus and may further magnify it by sharing the information themselves. Coordinated outputs can erode independence and diversity of inputs, particularly when platform features amplify social proof and herd signals; where governance or platform design reduces these incentives, effects may attenuate.</p><p>Collaborating agents can tailor misleading information to each sub-community&rsquo;s linguistic, cultural, and emotional markers, weaving segmented realities. These engineered realities can be designed to keep groups apart, making cross-cleavage consensus less feasible. Once initiated, such streams can spread via social contagion, with the effect of agents potentially cascading beyond direct connections.</p><p>By flooding the web with fabricated chatter, swarms can contaminate training data. This long-term &ldquo;LLM Grooming&rdquo; strategy allows adversaries to poison the epistemic substrate of AI. This threat is not theoretical: analysis of pro-Kremlin influence operations like the &ldquo;Pravda&rdquo; network suggests such tactics are already in use. These networks appear purpose-built for machine consumption. Duplication of articles across hundreds of domains, poor user interfaces, and low human traffic indicate their primary audience is web crawlers feeding LLMs. Operators deploy faux publics that flood the web. LLMs then ingest this chatter; at the next retraining cycle, fabricated narratives calcify in model weights [13]. Thus, AI swarms can rig the epistemic substrate on which future deliberation and future AI tools will rely, undermining the informed public deliberation on which democracy depends.</p><p>Separate from fragmentation, swarms can cheaply unleash coordinated synthetic harassment that relentlessly targets politicians, dissidents, academics, whistleblowers, journalists, and their networks with overwhelming, tailored abuse. Unlike conventional trolling, these swarms appear spontaneous while actually orchestrated by thousands of AI personas adapting to target responses. By the time monitoring teams distinguish AI campaigns from organic criticism, targets may have withdrawn from public life, delivering substantial victories for campaign operators while systematically excluding critical voices from democratic discourse.</p><p>As trust, already declining in many contexts, collapses, fear, uncertainty, and doubt (FUD) can drive users into gated channels and silence. When citizens realize that vast portions of online speech may be AI-generated, trust in platforms and users declines further. This shift is underway and has mixed implications: private groups can improve context, norms, and safety. Yet, they may reduce cross-cutting exposure, interfere with democratic speech, and transfer moderation from public to private actors – a trade-off that avoids centralized state control but raises concerns about opacity and uneven enforcement.</p><p>Some threat actors may even welcome their synthetic interventions being exposed, reasoning that exposing manipulation can sow as much confusion as successful deception. Compounding this, users may be misidentified as bots, weaponizing false accusations to discredit individuals and intensify FUD. This &ldquo;epistemic vertigo&rdquo; may mesh with low-cost LLM spam that overwhelms social media feeds, making human conversation harder to find. Together, FUD and content saturation could drive disengagement, shrinking the shared public sphere on which democracy relies. This trajectory is constrained by a critical boundary condition: given that mass user disengagement threatens platforms&rsquo; business models that depend on engagement, they will be incentivized to intervene. Their objective, however, would likely not be elimination but calibration to balance maximum engagement with stability.</p><p>Algorithmic over-compensation can then elevate celebrity and elite voices while sidelining ordinary citizens. When feeds flood with AI-authored posts, both ranking algorithms and users may retreat to trust proxies, such as the number of followers, official verification badges, and pre-existing traditional fame. Attention may concentrate around influencers, political elites, celebrities, and major brands, while ordinary participants fade. The public sphere contracts from many-to-many dialogue back to a few-to-many broadcast, eroding democratic pluralism and encouraging cynicism or migration to closed groups. Simultaneously, renewed public trust in professional journalism and greater reliance on accountable, verified outlets can improve attribution and reduce noise. Thus, whether this concentration of attention and influence represents a democratic loss or resilience gain may depend on access, pluralism, and transparency within those institutions.</p><p>Swarms may tip norms into action or dampen conformity, accelerating anti-democratic action [14]. Rather than occupying central or influential positions, these agents could operate on the periphery of social networks, where early mobilization often begins [15]. Similar strategies can be weaponized for micro-targeted voter suppression or mobilization. Reinforcement-learning agents could run thousands of experiments per hour, iteratively adjusting content while mining engagement and responses to infer voting intent and tactic success.</p><p>Taken to extremes, coordinated doubt may corrode institutional legitimacy and invite &ldquo;emergency&rdquo; rule. By coordinating subtle, growing doubts about electoral commissions, courts, or statistics bureaus, swarms could corrode procedural trust. As confidence falters, &ldquo;emergency&rdquo; measures (e.g., postponing elections, rejecting certified results) may become palatable, especially if deepfake endorsements from fabricated civic leaders amplify the call.</p><h2 id=governance-measures-and-technical-defenses>Governance Measures and Technical Defenses</h2><p>The emergence of AI swarms marks a critical juncture. Causality runs both ways: swarms endanger democratic norms, and governance quality shapes how potent or containable swarms become. While the escalating harms may lead some to advocate for abandoning these platforms altogether, their integration into modern social, political, and economic life makes widespread disengagement unlikely. The challenge we focus on here, therefore, is not how to dismantle platforms, but how to fortify them against manipulation for those who will continue to rely on them. Addressing this threat requires a multi-layered approach, yet we recognize that any proposed solution faces considerable political hurdles. Domestic political elites are often among the most prolific sources of misleading or manipulative information and may be unwilling to constrain technologies they perceive as beneficial to their own campaigns and objectives. Furthermore, technology companies and their leaders may refuse to implement meaningful changes because they prioritize expansion over safety and for fear of alienating major political actors and facing partisan backlash. These political challenges are compounded by complex issues of jurisdiction and enforcement.</p><p>Distinguishing malicious AI coordination from genuine, often bursty human grassroots coordination is a challenge. The line blurs further in grey areas where personal AI could have benign applications. For instance, tools used with clear disclosure and without impersonation might broaden civic engagement by helping users overcome barriers like language proficiency or lack of time. This raises a critical question: why can&rsquo;t pro-social swarms simply counter malicious ones in a symmetrical arms race? The digital attention economy often rewards content that triggers outrage, fear, and group identity (the primary tools of manipulators), making it more viral than nuanced or civil messages. Furthermore, pro-social actors are bound by ethical constraints against using the tactics (deception, impersonation, and emotional exploitation of human biases) that make malicious swarms effective. These factors might skew the emerging social dynamics in a negative direction.</p><p>Defense is a persistent arms race between detection and evasion. Therefore, the primary goal of technical defenses is not foolproof prevention but to raise the stakes for attackers by increasing their operational complexity and resource requirements, while making discovery both more likely and more costly for them. The first line of defense should be always-on detection with public audits. Platforms and regulators could require continuous, real-time monitoring detectors that scan live traffic for statistically anomalous coordination patterns – the imperfect fingerprints of inauthentic swarms [11]. This focus on inauthentic behavior (i.e., provenance and coordination), rather than the semantic content of speech, would avoid the intractable role of a central arbiter of truth. By prioritizing procedural legitimacy (authentic, independent actors) over semantic truth, this framework sidesteps the deep epistemic question of who determines misinformation (note, however, that professional fact-checkers have proven to be remarkably accurate and consistent). Advanced analytics can (i) identify emergent agent clusters by surfacing camouflaged indicators of coordinated activity; and (ii) spot narrative-alignment drifts. However, attackers will inevitably adapt, for instance, by training swarms to mimic the statistical patterns of genuine grassroots mobilization, necessitating continuous evolution of defenses.</p><p>Deploying these detection systems would require mandates, audits, and transparency to prevent misuse. Relying purely on voluntary measures may be insufficient as the assumption that market forces alone will punish platforms overlooks critical market failures. Platforms often face misaligned incentives, since inauthentic accounts can inflate the engagement metrics that drive revenue, while users frequently cannot distinguish sophisticated bots from genuine activity, preventing them from effectively &lsquo;voting with their feet&rsquo;. However, acknowledging market failure should not obscure the symmetric risk of government failure. Poorly designed mandates could be politically weaponized to selectively punish platforms, enforced via biased judgment calls, or implemented in ways that preemptively stifle architectural innovation (such as decentralized, protocol-based approaches). For this reason, compliance may be mandated and enforced through commercial-incentive levers, such as delisting non-compliant platforms from ad markets or app stores, thereby shifting from voluntary promises to financial consequences.</p><p>To extend protection to end users, platforms should offer optional &ldquo;AI shields.&rdquo; Shields could label posts that carry high swarm-likelihood scores, let users down-rank or hide them, and surface short provenance explanations in situ. Local scoring would preserve privacy while giving citizens agency over their information diets. Aggregated, anonymized feedback can be shared publicly, forming a distributed early-warning grid, yet this system remains vulnerable to adversarial manipulation by swarms programmed to whitelist their own propaganda and blacklist legitimate opponents through false reporting.</p><p>Simulation can stress-test detectors. Real-time monitors would be effective only when they anticipate future tactics. Because defenders lack access to the autonomous and evolving decision-making logic of AI swarms, agent-based simulation may be the only reliable window into how these systems behave. AI agents seeded into synthetic networks can replicate a platform&rsquo;s graph structure, content cadence, and recommender logic, yielding traces to recalibrate detectors. By repeatedly testing defenses against simulated swarms, researchers could identify the limits of their persuasive power, uncover their longer-term strategies, and reinforce protective measures.</p><p>Where manipulation slips through, calibrated defensive agents could deploy watermarked counter-narratives overtly labeled to clearly attribute them to their source. This is perhaps the most perilous countermeasure, as state-sanctioned tools for speech intervention are inherently political and risky. In the hands of a government, such tools could suppress dissent or amplify incumbents. Therefore, the deployment of defensive AI can only be considered if governed by strict, transparent, and democratically accountable frameworks. These must include independent oversight, publicly auditable criteria for what constitutes a manipulative campaign, and clear, unambiguous watermarking of all defensive content. Under such strict governance, defensive AI agents can disseminate accurate information, warn targeted communities, and promote media literacy at scale [6]. Crucially, while acknowledging the asymmetric battlefield, such counter-narratives need not be merely reactive; they could also be deployed proactively to inoculate communities against emerging threats, aiming to minimize polarization and misinformation before a campaign takes hold. Counter-messaging must prioritize precision over volume; if defensive agents indiscriminately flood a platform, human voices could vanish into synthetic content, triggering the collapse we seek to avert. Thus, defensive AI should intervene only where manipulation is detected and verified.</p><p>The adaptive nature of AI swarms underscores the need for a complementary approach: strengthening provenance. Stronger provenance may reinforce the reliability of identity signals without muting speech. Policymakers may incentivize the rapid adoption of passkeys, cryptographic attestations, and federated reputation standards, backed by anti-spoofing R&amp;D. However, &ldquo;proof-of-human&rdquo; is no panacea: millions of people online lack identification, biometrics raise privacy risks, and verified accounts can be hijacked. Real-identity policies may deter bots, yet endanger political dissidents, activists, and whistleblowers who rely on anonymity to speak safely. Nevertheless, provenance strengthening is among the most promising ways to raise the cost of mass manipulation. Safeguards could allow verified-yet-anonymous posting, periodic re-verification to curb hijacking, and symbolic subscription fees to deter botnets. Cryptographic tools can further protect privacy while preserving accountability.</p><p>To counter the speed, scale, independence, and adaptability of AI swarms, a step toward global coordination could be a distributed &lsquo;AI Influence Observatory&rsquo; ecosystem – a network of academic groups, NGOs, and multilateral institutions. Its goal would be to standardize evidence, improve situational awareness, and enable faster collective response rather than impose top-down reputational penalties. To be practical, the ecosystem should rely on narrowly defined, privacy-preserving inputs and provide vetted researcher sandboxes for independent analysis. Civil-society reporting, investigative journalism, and whistleblower channels would complement technical signals, enabling triangulation across diverse evidence streams. For severe cross-border incidents, an impartial multilateral investigatory mechanism could evaluate claims and publish verified incident reports. The observatory&rsquo;s verified incident reports could then serve as an impartial evidence base, enabling national or regional regulators to more effectively apply their own enforcement actions and economic sanctions.</p><p>Because regulation and voluntary compliance face considerable political resistance, and because AI swarms make sophisticated manipulation cheaper and more effective, a pragmatic approach should target underlying economic drivers. A key priority here would be to disrupt the commercial market that underpins large-scale manipulation, where private sellers offer services ranging from boosting vanity metrics to executing coordinated influence operations at remarkably low costs. Beyond detection, commercial-incentive levers can reduce profits from manipulation by domestic and foreign operators. Policies that may be helpful include adopting no-revenue policies for malicious swarm-proliferated content, discounting synthetic engagement in ranking and revenue-sharing, and publishing audited bot-traffic metrics. Safeguards must cover parties, campaigns, and officeholders, including party-linked media and contractors.</p><p>Finally, companies should be required to promptly disclose when an account is flagged for behavior indicative of coordinated inauthentic activity, ensuring transparency while allowing for processes to address potential false positives. Policymakers should encourage – and where appropriate, incentivize – platforms to provide meaningful, privacy-preserving access for independent researchers so research can keep pace with evolving threats. At the same time, pre-bunking campaigns can help build cognitive resilience by empowering people and systems (&ldquo;model immunization&rdquo;) to spot the fingerprints of AI swarms. To strengthen structural defenses, interoperable &ldquo;pro-user media&rdquo; architectures, defined by empowering design principles that prioritize user well-being and epistemic health over maximizing viral engagement, can promote healthier information flows. At the same time, governments and technology firms should prioritize AI-safety research and fund independent measurement of misuse and societal impact. Taken together, these measures offer a layered strategy: immediate transparency to restore trust, proactive education to bolster citizens, resilient infrastructures to reduce systemic vulnerabilities, and sustained investment to monitor and adapt over time.</p><p>The next few years give an opportunity to proactively manage the challenges of the next generation of AI-enabled influence operations. If platforms deploy swarm detectors, frontier labs submit models to standardized persuasion &ldquo;stress-tests,&rdquo; and governments launch an AI Influence Observatory that publishes open incident telemetry, we may be able to mitigate the most significant risks before key political future events, without freezing innovation. Doing so will require rapid iteration, data-sharing, and coordination across scientific, civil society, industry, and election security. Success depends on fostering collaborative action without hindering scientific research, while ensuring that the public sphere remains both resilient and accountable. By committing now to rigorous measurement, proportionate safeguards, and shared oversight, upcoming elections could even become a proving ground for, rather than a setback to, democratic AI governance.</p><hr><p><a href=footnotes/>View Footnotes</a> | <a href=/>Back to Library</a></p><section><h2>Contents</h2><ul><li><a href=/library/how-malicious-ai-swarms-can-threaten-democracy/footnotes/>Footnotes - How malicious AI swarms can threaten democracy</a></li></ul></section></article></main></body></html>